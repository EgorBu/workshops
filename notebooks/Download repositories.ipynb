{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data\n",
    "\n",
    "Since we plan to analyze a few repositories in this workshop, let's download them.\n",
    "\n",
    "We'll first get metadata about a user or organization thanks to GitHub API, and then download the repositories that interest us the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving metadata about a user/organization\n",
    "\n",
    "We iterate as long as the API gives us a pointer to another response page. We filter forks to focus on original repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dump as json_dump\n",
    "from re import compile as re_compile\n",
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "# Generate a personal access token here: https://github.com/settings/tokens\n",
    "TOKEN = # See comment above, please generate a token and put it here\n",
    "\n",
    "\n",
    "next_pattern = re_compile('<(https://api.github.com/user/[^/]+/repos\\?[^>]*page=\\d+[^>]*)>; rel=\"next\"')\n",
    "\n",
    "\n",
    "def parse_next(link_header: str) -> Optional[str]:\n",
    "    match = next_pattern.search(link_header)\n",
    "    return match.group(1) if match is not None else None\n",
    "\n",
    "\n",
    "def list_repositories(user: str):\n",
    "    headers = dict(Authorization=\"token {token}\".format(token=TOKEN))\n",
    "    url = \"https://api.github.com/users/{user}/repos\".format(user=user)\n",
    "    while url is not None:\n",
    "        request = requests.get(url, headers=headers)\n",
    "        request.raise_for_status()\n",
    "        for repo in request.json():\n",
    "            if not repo[\"fork\"]:\n",
    "                yield repo[\"name\"], repo[\"clone_url\"], repo[\"size\"], repo[\"stargazers_count\"]\n",
    "        url = parse_next(request.headers[\"Link\"])\n",
    "\n",
    "\n",
    "with open('output/repos.json', 'w') as fh:\n",
    "    json_dump(list(list_repositories(\"apache\")), fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for repos we want to analyze\n",
    "\n",
    "We'll keep the most popular repos by stars that are under a given size threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import load as json_load\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "MAX_SIZE = 50 * 1024\n",
    "\n",
    "\n",
    "filtered = []\n",
    "with open('output/repos.json', 'r') as fh:\n",
    "    repos = json_load(fh)\n",
    "    filtered = [(name, clone_url)\n",
    "                for name, clone_url, size, _ in sorted(repos, key=itemgetter(3), reverse=True)\n",
    "                if size <= MAX_SIZE]\n",
    "\n",
    "\n",
    "pprint(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "\n",
    "PARALLEL_DOWNLOADS = 10\n",
    "REPOS_NUMBER = 50\n",
    "\n",
    "\n",
    "def clone_repo(clone_url: str):\n",
    "    !cd /devfest/repos && git clone -q {clone_url}\n",
    "\n",
    "\n",
    "with ThreadPool(PARALLEL_DOWNLOADS) as pool:\n",
    "    pool.map(clone_repo, [clone_url for _, clone_url in filtered[:REPOS_NUMBER]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
