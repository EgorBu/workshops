{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project and Developer Similarity\n",
    "\n",
    "In this notebook, we study what projects and developers are about and how to find projects or devs that are close to them.\n",
    "\n",
    "We use [Topic Modeling](https://en.wikipedia.org/wiki/Topic_model), through the excellent [BigARTM](http://docs.bigartm.org/en/stable/index.html) library to achieve that. Roughly speaking, the topic model we use sees each code file as stemming from some topics (e.g., `setup.py` might come from topics about packaging and documentation).\n",
    "\n",
    "To be able to apply this topic modeling technique, we need to transform each code file into a bag of identifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the paths we'll use for our inputs and outputs later in this notebook.\n",
    "\n",
    "A note on how we use the cells in this notebook: __all cells should be only dependent from the first one (that defines paths)__. It means we will \n",
    "save and load all results in files to achieve that. This helps mitigate the problems that arise from stateful notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from os import makedirs\n",
    "from os.path import join as path_join\n",
    "from typing import Union\n",
    "\n",
    "class Files(Enum):\n",
    "    IDENTIFIERS = [\"identifiers.jsonl.bz2\"]\n",
    "    SPLIT_IDENTIFIERS = [\"split-identifiers.jsonl.bz2\"]\n",
    "    FILTERED_IDENTIFIERS = [\"filtered-identifiers.jsonl.bz2\"]\n",
    "    IDENTIFIERS_COUNTER = [\"identifiers-counter.pickle\"]\n",
    "    COMMON_IDENTIFIERS_COUNTER = [\"common-identifiers-counter.pickle\"]\n",
    "    VW_DATASET = [\"dataset.vw\"]\n",
    "    ARTM_DICT = [\"bigartm\", \"identifiers.dict\"]\n",
    "    ARTM_STAGE1 = [\"bigartm\", \"stage1.model\"]\n",
    "    ARTM_STAGE2 = [\"bigartm\", \"stage2.model\"]\n",
    "    ARTM_FILES_TOPICS = [\"bigartm\", \"files-topics.bigartm\"]\n",
    "    ARTM_TOPICS_IDENTIFIERS = [\"bigartm\", \"topics-identifiers.bigartm\"]\n",
    "    PYLDAVIS_DATA = [\"pyldavis-data.pickle\"]\n",
    "\n",
    "\n",
    "class Dirs(Enum):\n",
    "    ARTM_LOGS = [\"bigartm\", \"logs\"]\n",
    "    ARTM_BATCHES = [\"bigartm\", \"batches\"]\n",
    "\n",
    "\n",
    "class Run:\n",
    "    def __init__(self, run_name: str):\n",
    "        self._run_name = run_name\n",
    "\n",
    "    def path(self, file_or_dir: Union[Files, Dirs]):\n",
    "        if isinstance(file_or_dir, Files):\n",
    "            dir_path = path_join(self._run_name, *file_or_dir.value[:-1])\n",
    "        elif isinstance(file_or_dir, Dirs):\n",
    "            dir_path = path_join(self._run_name, *file_or_dir.value)\n",
    "        makedirs(dir_path, exist_ok=True)\n",
    "        return (dir_path\n",
    "                if isinstance(file_or_dir, Dirs)\n",
    "                else path_join(dir_path, file_or_dir.value[-1]))\n",
    "\n",
    "\n",
    "run = Run(\"full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by the bulk of the preprocessing: extracting identifiers with [gitbase](http://docs.bigartm.org/en/stable/index.html). Since gitbase exposes any codebase as a relational database, we can extract what we wish with a SQL query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bz2 import open as bz2_open\n",
    "from json import dumps as json_dumps, loads as json_loads\n",
    "\n",
    "from utils import SUPPORTED_LANGUAGES, query_gitbase\n",
    "\n",
    "\n",
    "def extract_identifiers(identifiers_path: str, limit: int = 0):\n",
    "    sql = \"\"\"\n",
    "        SELECT\n",
    "            repository_id,\n",
    "            LANGUAGE(file_path) AS lang,\n",
    "            file_path,\n",
    "            uast_extract(\n",
    "                uast(blob_content,\n",
    "                     LANGUAGE(file_path),\n",
    "                     '//uast:Identifier'),\n",
    "                'Name'\n",
    "            ) AS identifiers\n",
    "        FROM refs\n",
    "        NATURAL JOIN commit_files\n",
    "        NATURAL JOIN blobs\n",
    "        WHERE\n",
    "            ref_name = 'HEAD'\n",
    "            AND NOT IS_VENDOR(file_path)\n",
    "            AND NOT IS_BINARY(file_path)\n",
    "            AND LANGUAGE(file_path) IN (%s)\n",
    "        %s\n",
    "    \"\"\" % (\n",
    "        \",\".join(\"'%s'\" % language for language in SUPPORTED_LANGUAGES),\n",
    "        \"LIMIT %d\" % limit if limit > 0 else \"\"\n",
    "    )\n",
    "\n",
    "    with bz2_open(identifiers_path, \"wt\", encoding=\"utf8\") as fh:\n",
    "        for row in query_gitbase(sql):\n",
    "            if row[\"identifiers\"] is None:\n",
    "                continue\n",
    "            # for key, value in row.items():\n",
    "            #     row[key] = value.decode(\"utf8\", \"replace\")\n",
    "            row[\"identifiers\"] = json_loads(row[\"identifiers\"])\n",
    "            fh.write(\"%s\\n\" % json_dumps(row))\n",
    "\n",
    "\n",
    "extract_identifiers(run.path(Files.IDENTIFIERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a file that stores all the identifiers, we can refine it until it is ready for topic modeling! The remaining steps are to further split each identifier (`set_timer` should become `set` and `timer`), and to apply some stemming (`connecting` and `connection` should both result in `connect`, note that the result stem might not be an English word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bz2 import open as bz2_open\n",
    "from collections import Counter\n",
    "from json import dumps as json_dumps, loads as json_loads\n",
    "from pickle import dump as pickle_dump\n",
    "\n",
    "from utils import TokenParser\n",
    "\n",
    "\n",
    "def split_identifiers(identifiers_path: str,\n",
    "                      split_identifiers_path: str,\n",
    "                      counter_path: str):\n",
    "    with bz2_open(identifiers_path, \"rt\", encoding=\"utf8\") as fh_identifiers, \\\n",
    "            bz2_open(split_identifiers_path, \"wt\", encoding=\"utf8\") as fh_split_identifiers, \\\n",
    "            open(counter_path, \"wb\") as fh_counter:\n",
    "        identifiers_counter = Counter()\n",
    "        token_parser = TokenParser()\n",
    "        for row_str in fh_identifiers:\n",
    "            row = json_loads(row_str)\n",
    "            identifiers = row.pop(\"identifiers\")\n",
    "            split_identifiers = []\n",
    "            for identifier in identifiers:\n",
    "                split_identifiers.extend(token_parser(identifier))\n",
    "            identifiers_counter.update(split_identifiers)\n",
    "            row[\"split_identifiers\"] = split_identifiers\n",
    "            fh_split_identifiers.write(\"%s\\n\" % json_dumps(row))\n",
    "        pickle_dump(identifiers_counter, fh_counter)\n",
    "\n",
    "\n",
    "split_identifiers(run.path(Files.IDENTIFIERS),\n",
    "                  run.path(Files.SPLIT_IDENTIFIERS),\n",
    "                  run.path(Files.IDENTIFIERS_COUNTER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting identifiers still need some processing: some of them appear only a few times and will bring mostly noise to our models. We will discard them now. The first step is to find out which identifiers are common enough to be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump as pickle_dump, load as pickle_load\n",
    "\n",
    "\n",
    "def build_common_counter(count_threshold: int,\n",
    "                         counter_path: str,\n",
    "                         common_counter_path: str):\n",
    "    with open(counter_path, \"rb\") as fh:\n",
    "        identifiers_counter = pickle_load(fh)\n",
    "    print(\"Found %d different identifiers\" % len(identifiers_counter))\n",
    "\n",
    "    common_identifiers_counter = identifiers_counter.copy()\n",
    "    for identifier, count in identifiers_counter.items():\n",
    "        if count < count_threshold:\n",
    "            del common_identifiers_counter[identifier]\n",
    "    with open(common_counter_path, \"wb\") as fh:\n",
    "        pickle_dump(common_identifiers_counter, fh)\n",
    "    print(\"Found %d different identifiers after pruning\"\n",
    "          % len(common_identifiers_counter))\n",
    "\n",
    "\n",
    "build_common_counter(10,\n",
    "                     run.path(Files.IDENTIFIERS_COUNTER),\n",
    "                     run.path(Files.COMMON_IDENTIFIERS_COUNTER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know which are the common identifiers, we can recreate our mapping from files to identifiers with only the ones that we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps as json_dumps, loads as json_loads\n",
    "from pickle import load as pickle_load\n",
    "\n",
    "\n",
    "def filter_identifiers(split_identifiers_path: str,\n",
    "                       common_counter_path: str,\n",
    "                       filtered_identifiers_path: str):\n",
    "    with bz2_open(split_identifiers_path, \"rt\", encoding=\"utf8\") as fh_split_identifiers, \\\n",
    "            open(common_counter_path, \"rb\") as fh_common_counter, \\\n",
    "            bz2_open(filtered_identifiers_path, \"wt\", encoding=\"utf8\") as fh_filtered_identifiers:\n",
    "        common_identifiers_counter = pickle_load(fh_common_counter)\n",
    "        for row_str in fh_split_identifiers:\n",
    "            row = json_loads(row_str)\n",
    "            row[\"split_identifiers\"] = [identifier\n",
    "                                        for identifier in row[\"split_identifiers\"]\n",
    "                                        if identifier in common_identifiers_counter]\n",
    "            if row[\"split_identifiers\"]:\n",
    "                fh_filtered_identifiers.write(\"%s\\n\" % json_dumps(row))\n",
    "\n",
    "\n",
    "filter_identifiers(run.path(Files.SPLIT_IDENTIFIERS),\n",
    "                   run.path(Files.COMMON_IDENTIFIERS_COUNTER),\n",
    "                   run.path(Files.FILTERED_IDENTIFIERS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing is over! We now create the input dataset, in the VW format (see https://bigartm.readthedocs.io/en/stable/tutorials/datasets.html). We replace spaces in `file_path` to avoid creating false identifiers (VW would consider the latter parts of a path containing spaces to be identifiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from os.path import join as path_join\n",
    "\n",
    "\n",
    "def build_vw_dataset(filtered_identifiers_path: str,\n",
    "                     vw_dataset_path: str):\n",
    "    with bz2_open(filtered_identifiers_path, \"rt\", encoding=\"utf8\") as fh_filtered_identifiers, \\\n",
    "            open(vw_dataset_path, \"w\") as fh_vw:\n",
    "        for row_str in fh_filtered_identifiers:\n",
    "            counter = Counter()\n",
    "            row = json_loads(row_str)\n",
    "            counter.update(row[\"split_identifiers\"])\n",
    "            fh_vw.write(\"%s//%s//%s %s\\n\" % (\n",
    "                row[\"repository_id\"],\n",
    "                row[\"lang\"],\n",
    "                row[\"file_path\"].replace(\" \", \"_\"),\n",
    "                \" \".join(\"%s:%d\" % (identifier, count)\n",
    "                     for identifier, count in counter.items())\n",
    "            ))\n",
    "\n",
    "\n",
    "build_vw_dataset(run.path(Files.FILTERED_IDENTIFIERS),\n",
    "                 run.path(Files.VW_DATASET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigartm has its own binary format to efficiently store and access the data used to build its topic models. The next step is therefore to transform our VW dataset into the correct Bigartm format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bigartm(vw_dataset_path: str,\n",
    "                    artm_batches_path: str,\n",
    "                    artm_dict_path: str,\n",
    "                    artm_logs_path: str):\n",
    "    !rm -rf {artm_batches_path} {artm_dict_path}\n",
    "    !bigartm \\\n",
    "        --log-dir {artm_logs_path} \\\n",
    "        -c {vw_dataset_path} \\\n",
    "        -p 0 \\\n",
    "        --save-batches {artm_batches_path} \\\n",
    "        --save-dictionary {artm_dict_path}\n",
    "\n",
    "\n",
    "prepare_bigartm(run.path(Files.VW_DATASET),\n",
    "                run.path(Dirs.ARTM_BATCHES),\n",
    "                run.path(Files.ARTM_DICT),\n",
    "                run.path(Dirs.ARTM_LOGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train our first topic model! As per the Bigartm documentation, we don't use too much magic (yet), and only use one regularizer --- the decorrelation one. It will make sure that no 2 topics are about the same concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "def train_topic_model(artm_batches_path: str,\n",
    "                      artm_dict_path: str,\n",
    "                      artm_stage1_path: str,\n",
    "                      artm_logs_path: str,\n",
    "                      n_topics: int = 64,\n",
    "                      n_epochs: int = 100,\n",
    "                      n_cpus: int = cpu_count() * 2,\n",
    "                      seed: int = 2019,\n",
    "                      regularizer: str = '\"1000 Decorrelation\"'):\n",
    "    !bigartm \\\n",
    "        --log-dir {artm_logs_path} \\\n",
    "        --use-batches {artm_batches_path} \\\n",
    "        --use-dictionary {artm_dict_path} \\\n",
    "        -t {n_topics} \\\n",
    "        -p {n_epochs} \\\n",
    "        --threads {n_cpus} \\\n",
    "        --rand-seed {seed} \\\n",
    "        --regularizer {regularizer} \\\n",
    "        --save-model {artm_stage1_path} \\\n",
    "        --force\n",
    "\n",
    "\n",
    "train_topic_model(run.path(Dirs.ARTM_BATCHES),\n",
    "                  run.path(Files.ARTM_DICT),\n",
    "                  run.path(Files.ARTM_STAGE1),\n",
    "                  run.path(Dirs.ARTM_LOGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This topic model is probably quite good already, but since Bigartm is a powerful library, we can improve it even further by making it sparser: documents and topics will be sharper, they will contain less words and topics and will focus on the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify_topic_model(\n",
    "    artm_batches_path: str,\n",
    "    artm_dict_path: str,\n",
    "    artm_stage1_path: str,\n",
    "    artm_stage2_path: str,\n",
    "    artm_files_topics_path: str,\n",
    "    artm_topics_identifiers_path: str,\n",
    "    artm_logs_path: str,\n",
    "    n_epochs: int = 20,\n",
    "    n_cpus: int = cpu_count() * 2,\n",
    "    seed: int = 2019,\n",
    "    regularizer: str = ' \"1000 Decorrelation\" \"0.5 SparsePhi\" \"0.5 SparseTheta\" '\n",
    "):\n",
    "    !bigartm \\\n",
    "        --log-dir {artm_logs_path} \\\n",
    "        --use-batches {artm_batches_path} \\\n",
    "        --use-dictionary {artm_dict_path} \\\n",
    "        --load-model {artm_stage1_path} \\\n",
    "        -p {n_epochs} \\\n",
    "        --threads {n_cpus} \\\n",
    "        --rand-seed {seed} \\\n",
    "        --regularizer {regularizer} \\\n",
    "        --save-model {artm_stage2_path} \\\n",
    "        --force \\\n",
    "        --write-predictions {artm_files_topics_path} \\\n",
    "        --write-model-readable {artm_topics_identifiers_path}\n",
    "\n",
    "\n",
    "sparsify_topic_model(run.path(Dirs.ARTM_BATCHES),\n",
    "                     run.path(Files.ARTM_DICT),\n",
    "                     run.path(Files.ARTM_STAGE1),\n",
    "                     run.path(Files.ARTM_STAGE2),\n",
    "                     run.path(Files.ARTM_FILES_TOPICS),\n",
    "                     run.path(Files.ARTM_TOPICS_IDENTIFIERS),\n",
    "                     run.path(Dirs.ARTM_LOGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our topic model should be perfectly cooked now. It's time to taste it. Let's visualize the topics with the great [pyLDAvis](https://github.com/bmabey/pyLDAvis) tool. To do that, we first extract the relevant info from our model. We use BigARTM and it's not supported out of the box so we have a bit of work to do. If we'd have used Gensim or some other better-known (not better) library, this step would be a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bz2 import open as bz2_open\n",
    "from json import loads as json_loads\n",
    "from pickle import dump as pickle_dump, load as pickle_load\n",
    "\n",
    "from numpy import ones as numpy_ones\n",
    "from pandas import read_csv as pandas_read_csv\n",
    "from pyLDAvis import prepare as pyldavis_prepare\n",
    "\n",
    "\n",
    "def prepare_visualization(artm_files_topics_path: str,\n",
    "                          artm_topics_identifiers_path: str,\n",
    "                          common_counter_path: str,\n",
    "                          filtered_identifiers_path: str,\n",
    "                          pyldavis_data_path: str):\n",
    "    topics_identifiers_df = pandas_read_csv(artm_topics_identifiers_path, delimiter=\";\")\n",
    "    files_topics_df = pandas_read_csv(artm_files_topics_path, delimiter=\";\")\n",
    "    topic_term_dists = topics_identifiers_df.iloc[:, 2:].values.T\n",
    "    doc_topic_dists = files_topics_df.iloc[:, 2:].values\n",
    "    doc_topic_dists /= doc_topic_dists.sum(axis=1, keepdims=1)\n",
    "    for i, row in enumerate(doc_topic_dists):\n",
    "        if not (0.9 < row.sum() < 1.1):\n",
    "            doc_topic_dists[i] = (numpy_ones((doc_topic_dists.shape[1],))\n",
    "                                  / doc_topic_dists.shape[1])\n",
    "    doc_index = files_topics_df[\"title\"].values\n",
    "    vocab = topics_identifiers_df[\"token\"].values\n",
    "    with bz2_open(filtered_identifiers_path, \"rt\", encoding=\"utf8\") as fh_rj, \\\n",
    "            open(common_counter_path, \"rb\") as fh_rp:\n",
    "        common_identifiers_counter = pickle_load(fh_rp)\n",
    "        doc_lengths_index = {}\n",
    "        for row_str in fh_rj:\n",
    "            row = json_loads(row_str)\n",
    "            doc_lengths_index[\n",
    "                \"%s//%s//%s\" % (\n",
    "                    row[\"repository_id\"],\n",
    "                    row[\"lang\"],\n",
    "                    row[\"file_path\"].replace(\" \", \"_\"))\n",
    "            ] = len(row[\"split_identifiers\"])\n",
    "        term_frequency = [common_identifiers_counter[t] for t in vocab]\n",
    "        doc_lengths = [doc_lengths_index[doc] for doc in doc_index]\n",
    "\n",
    "    with open(pyldavis_data_path, \"wb\") as fh:\n",
    "        pyldavis_data = pyldavis_prepare(topic_term_dists=topic_term_dists, \n",
    "                                         doc_topic_dists=doc_topic_dists,\n",
    "                                         doc_lengths=doc_lengths,\n",
    "                                         vocab=vocab,\n",
    "                                         term_frequency=term_frequency,\n",
    "                                         sort_topics=False)\n",
    "        pickle_dump(pyldavis_data, fh)\n",
    "\n",
    "\n",
    "prepare_visualization(run.path(Files.ARTM_FILES_TOPICS),\n",
    "                      run.path(Files.ARTM_TOPICS_IDENTIFIERS),\n",
    "                      run.path(Files.COMMON_IDENTIFIERS_COUNTER),\n",
    "                      run.path(Files.FILTERED_IDENTIFIERS),\n",
    "                      run.path(Files.PYLDAVIS_DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the topics we just learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load as pickle_load\n",
    "\n",
    "from pyLDAvis import display as pyldavis_display\n",
    "\n",
    "\n",
    "def visualize(pyldavis_data_path: str):\n",
    "    with open(pyldavis_data_path, \"rb\") as fh:\n",
    "        pyldavis_data = pickle_load(fh)\n",
    "    return pyldavis_display(pyldavis_data)\n",
    "\n",
    "\n",
    "visualize(run.path(Files.PYLDAVIS_DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projects search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our learned topic model, we can now tackle our first task: understand what projects are about and find similar projects to existing ones based on their topics.\n",
    "\n",
    "To do that, we will compute the distance between the topics of all projects, and return the closest ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bz2 import open as bz2_open\n",
    "from json import loads as json_loads\n",
    "from pickle import load as pickle_load\n",
    "from re import compile as re_compile\n",
    "\n",
    "from numpy import sum as np_sum, vectorize\n",
    "from pandas import read_csv as pandas_read_csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def build_projects_topics(artm_files_topics_path: str,\n",
    "                          artm_topics_identifiers_path: str):\n",
    "    files_topics_df = pandas_read_csv(artm_files_topics_path, delimiter=\";\")\n",
    "    topics_identifiers_df = pandas_read_csv(artm_topics_identifiers_path, delimiter=\";\").T\n",
    "    topics_topk = topics_identifiers_df.iloc[1:, :].values.argsort()[:, -10:][:, ::-1]\n",
    "    vocab = topics_identifiers_df.iloc[0, :].values\n",
    "    topics_top_words = vectorize(lambda x: vocab[x])(topics_topk)\n",
    "    all_but_repo_pattern = re_compile(r\"//.+//.+$\")\n",
    "    files_topics_df[\"repository_id\"] = files_topics_df[\"title\"].apply(\n",
    "        lambda x: all_but_repo_pattern.sub(\"\", x))\n",
    "    grouped_by_repo_df = files_topics_df.iloc[:, 2:].groupby(\"repository_id\")\n",
    "    topics = grouped_by_repo_df.aggregate(np_sum).values\n",
    "    topics /= topics.sum(axis=1, keepdims=1)\n",
    "    repos = [r for r, _ in grouped_by_repo_df]\n",
    "    repos_index = {r: i for i, r in enumerate(repos)}\n",
    "    for i, ((repo, _), topics_dist) in enumerate(zip(grouped_by_repo_df, topics)):\n",
    "        topk = topics_dist.argsort()[-3:][::-1]\n",
    "        probk = topics_dist[topk]\n",
    "        print(\"%s (%d):\\n%s\" % (repo, i, \"\\n\".join(\"  %.2f: %s\" % (prob, \", \".join(topics_top_words[top + 1]))\n",
    "                                                   for prob, top in zip(probk, topk))))\n",
    "    most_similar = cosine_similarity(topics).argsort()[:, -10:-1][:, ::-1]\n",
    "\n",
    "    def query_similar(repo: str):\n",
    "        return [repos[i] for i in most_similar[repos_index[repo]]]\n",
    "\n",
    "    return query_similar\n",
    "\n",
    "\n",
    "f = build_projects_topics(run.path(Files.ARTM_FILES_TOPICS),\n",
    "                          run.path(Files.ARTM_TOPICS_IDENTIFIERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(\"log4j\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
